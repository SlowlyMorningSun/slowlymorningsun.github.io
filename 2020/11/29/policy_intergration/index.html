<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>强化学习求解（一） ———— 动态规划法 | SlowlyMorningSun</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="强化学习求解（一） ———— 动态规划法&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习求解（一） ———— 动态规划法">
<meta property="og:url" content="http://example.com/2020/11/29/policy_intergration/index.html">
<meta property="og:site_name" content="SlowlyMorningSun">
<meta property="og:description" content="强化学习求解（一） ———— 动态规划法&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;">
<meta property="og:locale">
<meta property="article:published_time" content="2020-11-29T13:51:42.000Z">
<meta property="article:modified_time" content="2021-02-23T09:24:22.000Z">
<meta property="article:author" content="Xu，Chenyang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="SlowlyMorningSun" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">SlowlyMorningSun</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-policy_intergration" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/29/policy_intergration/" class="article-date">
  <time class="dt-published" datetime="2020-11-29T13:51:42.000Z" itemprop="datePublished">2020-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      强化学习求解（一） ———— 动态规划法
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="强化学习求解（一）-————-动态规划法"><a href="#强化学习求解（一）-————-动态规划法" class="headerlink" title="强化学习求解（一） ———— 动态规划法"></a>强化学习求解（一） ———— 动态规划法</h1><p>==========================================================================================================================</p>
<span id="more"></span>
<h2 id="关于主要的配置需求"><a href="#关于主要的配置需求" class="headerlink" title="关于主要的配置需求"></a>关于主要的配置需求</h2><ol>
<li>numpy</li>
<li>gym</li>
</ol>
<h2 id="2-关于程序的解释："><a href="#2-关于程序的解释：" class="headerlink" title="2.关于程序的解释："></a>2.关于程序的解释：</h2><p>对于本次的程序而言，主要分为三个部分的验证：<strong>策略评估</strong>，<strong>策略迭代</strong>，以及<strong>值迭代</strong>，其中本人认为值迭代相较于策略迭代更为好用一些（在修改策略迭代时一直得不到想要的结果，虽然最后好了）。接下来，我将分别介绍这三个部分的代码。</p>
<h4 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h4><p>对于策略评估而言，我认为策略评估就是对当前状态下所应用的策略的优劣程度的，因此他并不会生成对应的策略，相反他会生成我们后面进行策略设计的评价指标————状态值函数V（s）。在策略评估的过程中，我们不仅仅要考虑多种的状态，还需要考虑到多种动作。因此，在算法中不仅仅需要状态的prob，还需要对应动作的action_prob。最后对于整体的迭代，需要停止迭代的阈值theta,通过阈值我们能够确定策略最终的结果是否收敛到一个值。接下来我们看代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 策略评估：</span><br><span class="line">def policy_evaluation(policy, environment, discount_factor = 1.0, theta = 0.99):</span><br><span class="line">    # 迭代式策略评估</span><br><span class="line">    env = environment</span><br><span class="line"></span><br><span class="line">    # 初始化一个全0的值函数向量用于记录状态值</span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    </span><br><span class="line">    # 迭代开始，10000为最大迭代次数</span><br><span class="line">    for _ in range(10000):</span><br><span class="line">        delta = 0</span><br><span class="line"></span><br><span class="line">        # 对于HelloGrid 中的每一个状态都进行全备份</span><br><span class="line">        for s in range(env.nS):</span><br><span class="line">            v = 0</span><br><span class="line">            # 检查下一个有可能执行的动作</span><br><span class="line">            for a, action_prob in enumerate(policy[s]):</span><br><span class="line">                # 对于每一个动作检查下一个状态</span><br><span class="line">                [prob, next_state, reward, done] = env.P[s][a]</span><br><span class="line">                # 累计计算下一个动作价值的期望</span><br><span class="line">                v += action_prob * prob * (reward + discount_factor* V[next_state]) </span><br><span class="line">            # 选出变化最大的量</span><br><span class="line">            delta = max(delta, np.abs(v-V[s]))</span><br><span class="line">            V[s] = v</span><br><span class="line"></span><br><span class="line">        # 检查是否满足停止条件</span><br><span class="line">        if delta &lt;= theta:</span><br><span class="line">            break</span><br><span class="line">    return np.array(V)</span><br></pre></td></tr></table></figure>

<p><strong>其中值得注意的是，这里的theta将直接影响到迭代的次数，以及后续过程中策略迭代的结果</strong></p>
<h4 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h4><p>在策略迭代里，需进行的就是不断的进行策略评估与策略优化，在进行迭代的过程里，保证每一次的策略policy都是上一次的最优的结果，然后反复的迭代，直到最终的迭代出的策略与上一次的策略一致未知，我们就认为策略进行了收敛。接下来，我们将展示代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 策略迭代算法</span><br><span class="line">def policy_iteration(env, policy, discount_factor = 1.0):</span><br><span class="line">    while True:</span><br><span class="line">        # 评估当前的策略policy</span><br><span class="line">        # print(&quot;policy&quot;,np.reshape(np.argmax(policy,axis=1),env.shape))</span><br><span class="line">        V = policy_evaluation(policy,env,discount_factor)</span><br><span class="line">        # print(&quot;V&quot;,V.reshape(env.shape))</span><br><span class="line">        # policy标志位，当某种的策略更改后，该标志位为False</span><br><span class="line">        policy_stable = True</span><br><span class="line">        # 策略改进</span><br><span class="line">        for s in range(env.nS):</span><br><span class="line">            # 在当前状态和策略下，选择概率最高的动作</span><br><span class="line">            old_action = np.argmax(policy[s])</span><br><span class="line">            # 在当前状态和策略下，找到最优动作</span><br><span class="line">            action_values = np.zeros(env.nA)</span><br><span class="line">            for a in range(env.nA):</span><br><span class="line">                [prob, next_state, reward, done] = env.P[s][a]</span><br><span class="line">                action_values[a] += prob*(reward + discount_factor * V[next_state])</span><br><span class="line"></span><br><span class="line">                # 由于Grid World环境存在状态遇到陷阱X则停止，因此让状态值遇到陷阱则为负无穷，不参与计算</span><br><span class="line">                if done and next_state != 15:</span><br><span class="line">                    action_values[a] = float(&#x27;-inf&#x27;)</span><br><span class="line">            # 采用贪婪算法更新当前策略</span><br><span class="line">            best_action = np.argmax(action_values)</span><br><span class="line">            if old_action != best_action:</span><br><span class="line">                policy_stable = False</span><br><span class="line">            policy[s] = np.eye(env.nA)[best_action]</span><br><span class="line">        # 选择的动作不在变化，则代表策略已经稳定下来</span><br><span class="line">        if policy_stable:</span><br><span class="line">            return policy, V</span><br></pre></td></tr></table></figure>

<h4 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h4><p>在本片博客的最后，我将介绍动态规划求解强化学习的最后一种方法————值迭代，我认为值迭代相较于前面的策略迭代算法而言，既不需要考虑多轮迭代导致效率下降的问题，也不需要考虑策略初始化的随机问题，受theta的影响应该会小很多。因此，我比较推荐这种算法来进行求解强化学习的问题。接下来我们将展示代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"># 值迭代算法</span><br><span class="line">def calc_action_value(state, V, discount_factor = 1.0):</span><br><span class="line">    # 对于给定的状态s计算其动作的期望值</span><br><span class="line"></span><br><span class="line">    # 初始化动作值得期望向量[0,0,0,0]</span><br><span class="line">    A = np.zeros(env.nA)</span><br><span class="line"></span><br><span class="line">    # 遍历当前状态下得所有动作</span><br><span class="line">    for a in range(env.nA):</span><br><span class="line">        [prob, next_state,reward ,done ] =  env.P[state][a]</span><br><span class="line">        A[a] += prob *(reward + discount_factor * V[next_state])</span><br><span class="line"></span><br><span class="line">    return A</span><br><span class="line"></span><br><span class="line">def value_iteration(env, theta = 0.1, discount_factor = 1.0):</span><br><span class="line">    # 值迭代算法</span><br><span class="line"></span><br><span class="line">    # 初始化状态值</span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line"></span><br><span class="line">    # 迭代计算找到最优得状态值函数</span><br><span class="line">    for _ in range(50):</span><br><span class="line">        # 停止标志位</span><br><span class="line">        delta = 0</span><br><span class="line"></span><br><span class="line">        # 计算每个状态得状态值</span><br><span class="line">        for s in range(env.nS):</span><br><span class="line">            # 执行一次找到当前状态得动作期望</span><br><span class="line">            A = calc_action_value(s,V)</span><br><span class="line">            # 选择最好得动作期望作为新得状态值</span><br><span class="line">            best_action_value = np.max(A)</span><br><span class="line"></span><br><span class="line">            # 计算停止得标志位</span><br><span class="line">            delta = max(delta, np.abs(best_action_value - V[s]))</span><br><span class="line"></span><br><span class="line">            # 更新状态值函数</span><br><span class="line">            V[s] = best_action_value</span><br><span class="line">        </span><br><span class="line">        if delta &lt; theta:</span><br><span class="line">            break</span><br><span class="line">    </span><br><span class="line">    #输出最优策略：通过最优状态值函数找到确定性策略，并初始化策略</span><br><span class="line">    policy = np.zeros([env.nS,env.nA])</span><br><span class="line"></span><br><span class="line">    for s in range(env.nS):</span><br><span class="line">        # 执行一次找到当前状态值得最优状态值得动作期望A</span><br><span class="line">        A = calc_action_value(s,V)</span><br><span class="line">        # 选出最大得状态值作为最优动作</span><br><span class="line">        best_action = np.argmax(A)</span><br><span class="line">        policy[s, best_action] = 1.0</span><br><span class="line">    return policy,V </span><br></pre></td></tr></table></figure>

<p>在博客的最后，我将说明一下随机策略的生成的方式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 生成随机策略：</span><br><span class="line">random_policy = np.ones([env.nS,env.nA])/env.nA</span><br></pre></td></tr></table></figure>

<p>以上就是我近期的学习情况。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/29/policy_intergration/" data-id="ckrc6fps70004b8uo2clw41to" data-title="强化学习求解（一） ———— 动态规划法" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/12/08/montocarlo/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          强化学习求解（二） ———— 蒙特卡洛法
        
      </div>
    </a>
  
  
    <a href="/2020/11/29/hello_gird/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">强化学习的Hello World ———— Hello Grid</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/07/20/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2021/03/08/policyGradient/">强化学习求解（五） ———— 策略梯度法</a>
          </li>
        
          <li>
            <a href="/2021/02/23/value_function_approximate/">强化学习求解（四） ———— 值函数近似法</a>
          </li>
        
          <li>
            <a href="/2021/02/17/temporal_difference/">强化学习求解（三） ———— 时间差分法</a>
          </li>
        
          <li>
            <a href="/2020/12/08/montocarlo/">强化学习求解（二） ———— 蒙特卡洛法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Xu，Chenyang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>