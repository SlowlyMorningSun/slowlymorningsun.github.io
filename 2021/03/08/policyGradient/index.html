<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>强化学习求解（五） ———— 策略梯度法 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="强化学习求解（五） ———— 策略梯度法&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习求解（五） ———— 策略梯度法">
<meta property="og:url" content="http://example.com/2021/03/08/policyGradient/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="强化学习求解（五） ———— 策略梯度法&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-03-08T15:51:42.000Z">
<meta property="article:modified_time" content="2021-03-09T03:12:12.000Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-policyGradient" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/08/policyGradient/" class="article-date">
  <time class="dt-published" datetime="2021-03-08T15:51:42.000Z" itemprop="datePublished">2021-03-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      强化学习求解（五） ———— 策略梯度法
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="强化学习求解（五）-————-策略梯度法"><a href="#强化学习求解（五）-————-策略梯度法" class="headerlink" title="强化学习求解（五） ———— 策略梯度法"></a>强化学习求解（五） ———— 策略梯度法</h1><p>==========================================================================================================================</p>
<span id="more"></span>
<h2 id="关于主要的配置需求"><a href="#关于主要的配置需求" class="headerlink" title="关于主要的配置需求"></a>关于主要的配置需求</h2><ol>
<li>numpy</li>
<li>gym</li>
<li>tensorflow 2.4版本</li>
<li>pandas</li>
<li>matplotlib</li>
</ol>
<h2 id="2-关于程序的解释："><a href="#2-关于程序的解释：" class="headerlink" title="2.关于程序的解释："></a>2.关于程序的解释：</h2><p>对于本次程序而言最主要的是了解<strong>策略梯度</strong>的方法，以及对于相关的策略梯度算法的程序的实现，其中在基础的策略梯度法中，我们将介绍<strong>蒙特卡洛策略梯度</strong>以及<strong>演员-评论家策略梯度算法</strong>。（局部最优好烦人）接下来，我们将从最基本的开始介绍</p>
<h4 id="对于策略梯度法的概述"><a href="#对于策略梯度法的概述" class="headerlink" title="对于策略梯度法的概述"></a>对于策略梯度法的概述</h4><p>其实，在作者认为，策略梯度法是基于策略的强化学习的主要方式，策略梯度法直接参数化策略，参数化的策略不再是一个集合，而是一个函数，即通过函数近似直接你和策略Π。<br>π<sub>θ</sub>(a|s) = P[a|s,θ]</p>
<p>其中θ为策略梯度函数的权重参数向量，π<sub>θ</sub>(a|s)表示使用参数向量θ进行函数拟合获得策略函数，进而获得智能体在状态s下采取的行动。之后利用梯度上升算法，获取到智能体关于奖励的期望J(θ)最大，既满足公式<br>θ<sub>t+1</sub> = θ<sub>t</sub> + α∇J(θ<sub>t</sub>)</p>
<h4 id="关于策略梯度法与值函数近似法的区别"><a href="#关于策略梯度法与值函数近似法的区别" class="headerlink" title="关于策略梯度法与值函数近似法的区别"></a>关于策略梯度法与值函数近似法的区别</h4><p><strong>值函数近似法</strong>：在值函数近似法中，动作选择的策略是不变的，如固定使用epsilon-贪婪算法作为策略选择方法，即在时间步t的状态s<sub>t</sub>下，选择动作的方式是固定的。</p>
<p><strong>策略梯度法</strong>：在策略梯度法中，智能体会学习不同的策略。即在某个时间步t的状态s<sub>t</sub>下，根据动作的概率分布进行选择，且动作概率分布可能不断的调整</p>
<h4 id="策略梯度法的优缺点"><a href="#策略梯度法的优缺点" class="headerlink" title="策略梯度法的优缺点"></a>策略梯度法的优缺点</h4><p>优点：易收敛，能够高效处理连续动作空间的任务，能学习随机策略</p>
<p>缺点：通常收敛到局部最优解，而非全局最优解，策略评估效率低下，方差较高</p>
<h4 id="优化策略目标函数"><a href="#优化策略目标函数" class="headerlink" title="优化策略目标函数"></a>优化策略目标函数</h4><p>首先对于策略目标函数而言可分为：<strong>起始价值</strong>，<strong>平均价值</strong>，<strong>时间步平均奖励</strong>，由于策略梯度定律三者的策略梯度是相痛的，因而不影响后续的求解问题。</p>
<p>对于优化策略目标函数而言，对于评价函数可以分为Softmax策略和高斯策略。其中对于Softmax而言其主要是针对离散型强化学习问题，而高斯策略对应的是连续型的强化学习任务。</p>
<h4 id="蒙特卡洛策略梯度法"><a href="#蒙特卡洛策略梯度法" class="headerlink" title="蒙特卡洛策略梯度法"></a>蒙特卡洛策略梯度法</h4><p>对于蒙特卡洛而言策略梯度法，是指从蒙特卡洛法的经验轨迹开始，利用蒙特卡洛法对累计奖励的策略梯度进行优化。详细代码如下：</p>
<p>关于流程可以分为5个部分：</p>
<ol>
<li>将环境产生的状态作为输入，并根据一定的策略选择所需执行的动作</li>
<li>执行步骤1所产生的动作，并得到新的状态和奖励</li>
<li>记录当前经验轨迹产生的反馈信号，用于当前的经验轨迹结束后训练策略网络</li>
<li>在当前经验轨迹结束（游戏结束）后，利用步骤3记录的经验轨迹信息进行学习，并更新策略网络</li>
<li>把下一个时间步的状态赋给当前的时间步，并重新回到步骤1，将结果作为策略选择输入<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"># 蒙特卡洛策略梯度类</span><br><span class="line">class Monte_Carlo_Policy_Gradient():</span><br><span class="line">    # 蒙特卡洛策略梯度方法类</span><br><span class="line">    def __init__(self, env, num_episodes=200, learning_rate = 0.01, reward_decay = 0.95):</span><br><span class="line">        # 初始化参数</span><br><span class="line">        # 动作空间</span><br><span class="line">        self.nA = env.action_space.n</span><br><span class="line">        # 状态空间</span><br><span class="line">        self.nS = env.observation_space.shape[0]</span><br><span class="line">        # 声明环境</span><br><span class="line">        self.env = env</span><br><span class="line">        # 迭代次数</span><br><span class="line">        self.num_episodes = num_episodes</span><br><span class="line">        # 奖励衰减系数</span><br><span class="line">        self.reward_decay = reward_decay</span><br><span class="line">        # 网络学习率</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        # 记录所有的奖励</span><br><span class="line">        self.rewards = []</span><br><span class="line">        # 最小奖励阈值</span><br><span class="line">        self.RENDER_REWARD_MIN = 30</span><br><span class="line">        # 是否重新分配环境标志位</span><br><span class="line">        self.RENDER_ENV = False</span><br><span class="line"></span><br><span class="line">        # 初始化策略网络类</span><br><span class="line">        self.PG = PolicyGradient(n_x=self.nS, n_y = self.nA, learning_rate = self.learning_rate, reward_decay = self.reward_decay)</span><br><span class="line"></span><br><span class="line">        # 记录经验轨迹的长度和奖励</span><br><span class="line">        record_head = namedtuple(&quot;Stats&quot;, [&quot;episode_lengths&quot;,&quot;episode_rewards&quot;])</span><br><span class="line">        self.record = record_head(episode_lengths = np.zeros(num_episodes), episode_rewards = np.zeros(num_episodes))</span><br><span class="line">    </span><br><span class="line">    # 蒙特卡洛策略梯度算法</span><br><span class="line">    def mcpg_learn(self):</span><br><span class="line">        # 迭代经验轨迹的次数</span><br><span class="line">        for i_episode in range(self.num_episodes):</span><br><span class="line">            # 输出经验轨迹迭代信息</span><br><span class="line">            num_present = (i_episode+1) / self.num_episodes</span><br><span class="line">            print(&quot;Episode &#123;&#125; / &#123;&#125;&quot;.format(i_episode+1,self.num_episodes), end=&quot;&quot;)</span><br><span class="line">            # 信息输出</span><br><span class="line">            print(&quot;=&quot;*round(num_present*60))</span><br><span class="line"></span><br><span class="line">            # 初始化环境</span><br><span class="line">            # 环境reset</span><br><span class="line">            state = env.reset()</span><br><span class="line">            </span><br><span class="line">            # 初始化奖励为0</span><br><span class="line">            reward = 0</span><br><span class="line"></span><br><span class="line">            # 遍历经验轨迹</span><br><span class="line">            for t in itertools.count():</span><br><span class="line">                # 如果环境重置标志位为True，则对环境重置</span><br><span class="line">                if self.RENDER_ENV: env.render()</span><br><span class="line"></span><br><span class="line">                # 步骤1：跟据给定的状态，策略网络选择出相应的动作</span><br><span class="line">                action = self.PG.choose_action(state)</span><br><span class="line">                # 步骤2：环境执行动作给出反馈信号</span><br><span class="line">                next_state, reward, done, _ = env.step(action)</span><br><span class="line">                # 步骤3：记录环境反馈信号，用于策略网络的训练数据</span><br><span class="line">                self.PG.store_memory(state, action, reward)</span><br><span class="line"></span><br><span class="line">                # 更新记录的信息</span><br><span class="line">                self.record.episode_rewards[i_episode] += reward</span><br><span class="line">                self.record.episode_lengths[i_episode] = t</span><br><span class="line"></span><br><span class="line">                # 游戏结束</span><br><span class="line">                if done:</span><br><span class="line">                    # 计算本次经验轨迹所获得的累计奖励</span><br><span class="line">                    episode_rewards_sum = sum(self.PG.episode_rewards)</span><br><span class="line">                    self.rewards.append(episode_rewards_sum)</span><br><span class="line">                    max_reward = np.max(self.rewards)</span><br><span class="line"></span><br><span class="line">                    # 步骤4：结束游戏后对策略网络进行训练</span><br><span class="line">                    self.PG.learn()</span><br><span class="line"></span><br><span class="line">                    # 标准化输出信息</span><br><span class="line">                    print(&quot;reward:&#123;&#125;,max reward:&#123;&#125;, episode:&#123;&#125;\n&quot;.format(episode_rewards_sum, max_reward, t))</span><br><span class="line"></span><br><span class="line">                    # 如果历史最大奖励大于奖励的最小阈值，则重置环境标志位为True</span><br><span class="line">                    if max_reward &gt; self.RENDER_REWARD_MIN : self.RENDER_ENV = True</span><br><span class="line"></span><br><span class="line">                    # 退出本次经验轨迹</span><br><span class="line">                    break</span><br><span class="line">                # 步骤5：存储下一个状态作为新的状态记录</span><br><span class="line">                state = next_state</span><br><span class="line">        # 返回记录数据</span><br><span class="line">        return self.record </span><br></pre></td></tr></table></figure></li>
</ol>
<p>关于策略梯度内部的神经网络的构建部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">class PolicyGradient():</span><br><span class="line">    # 策略梯度强化学习类，使用一个3层的神经网络作为策略网络</span><br><span class="line">    def __init__(self, n_x, n_y, learning_rate=0.01, reward_decay=0.95, load_path=None, save_path=None):</span><br><span class="line">        # 策略梯度类构造函数，初始化相关参数</span><br><span class="line"></span><br><span class="line">        # 初始化参数</span><br><span class="line">        # 策略网络输入</span><br><span class="line">        self.n_x = n_x</span><br><span class="line">        # 策略网络输出</span><br><span class="line">        self.n_y = n_y</span><br><span class="line">        # 策略网络学习率</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        # 策略网络奖励衰减率</span><br><span class="line">        self.reward_decay = reward_decay</span><br><span class="line"></span><br><span class="line">        # 经验轨迹采样数据（s，a，r）</span><br><span class="line">        self.episode_states, self.episode_actions, self.episode_rewards = [],[],[]</span><br><span class="line">        # 建立策略网络</span><br><span class="line">        self.__build_network()</span><br><span class="line">        </span><br><span class="line">    def all_actf(self):</span><br><span class="line">        all_act = self.dense_out</span><br><span class="line">        # print(all_act)</span><br><span class="line">        return all_act</span><br><span class="line">    def reca_batch(self,a_batch):</span><br><span class="line">        a = a_batch</span><br><span class="line">        return a</span><br><span class="line">    def def_loss(self,label=reca_batch,logit=all_actf):  </span><br><span class="line">        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits(labels=label,logits=logit)</span><br><span class="line">        # loss = tf.reduce_mean(neg_log_prob * self.disc_norm_ep_reward)</span><br><span class="line">        # return loss</span><br><span class="line">        return neg_log_prob</span><br><span class="line"></span><br><span class="line">    def __build_network(self):</span><br><span class="line">        # 建立一个三层的神经网络</span><br><span class="line">        # 输入层</span><br><span class="line">        x = Input(shape= (self.n_x,))</span><br><span class="line">        # 定义层数的神经元</span><br><span class="line">        # 第一层的隐层神经元数</span><br><span class="line">        layer1_units = 10</span><br><span class="line">        # 第二层隐层神经元元数</span><br><span class="line">        layer2_unit = 10</span><br><span class="line">        # 输出层的神经元数</span><br><span class="line">        layer_output_units = self.n_y</span><br><span class="line">        # 定义第一层</span><br><span class="line">        dense1 = Dense(layer1_units, activation = &#x27;relu&#x27;, name=&#x27;layer1&#x27;)(x)</span><br><span class="line">        # 定义第二层</span><br><span class="line">        dense2 = Dense(layer2_unit, activation= &#x27;relu&#x27;, name=&#x27;layer2&#x27;)(dense1)</span><br><span class="line">        # 定义输出层</span><br><span class="line">        self.dense_out = Dense(layer_output_units,name=&#x27;out_layer&#x27;)(dense2)</span><br><span class="line">        # softmax 输出</span><br><span class="line">        self.outputs_softmax = Softmax()(self.dense_out)</span><br><span class="line"></span><br><span class="line">        self.model = Model(x,self.outputs_softmax)</span><br><span class="line"></span><br><span class="line">        # # 定义loss函数</span><br><span class="line">        # def ccloss(label=self.episode_actions,logit=self.dense_out):</span><br><span class="line">        #     # print(logit)</span><br><span class="line">        #     # print(label)</span><br><span class="line">            </span><br><span class="line">        #     neg_log_prob = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=label)</span><br><span class="line">        #     # print(neg_log_prob)</span><br><span class="line">        #     print(self.disc_norm_ep_reward)</span><br><span class="line">        #     loss = tf.reduce_mean(neg_log_prob * self.disc_norm_ep_reward)</span><br><span class="line">        #     return loss</span><br><span class="line"></span><br><span class="line">        # 定义神经网络的损失函数与训练方式</span><br><span class="line">        self.model.compile(optimizer=Adam(learning_rate=self.lr),loss=self.def_loss)</span><br><span class="line"></span><br><span class="line">        self.model.summary()</span><br><span class="line"></span><br><span class="line">    def __disc_and_norm_rewards(self):</span><br><span class="line">        out = np.zeros_like(self.episode_rewards)</span><br><span class="line">        dis_reward = 0</span><br><span class="line"></span><br><span class="line">        for i in reversed(range(len(self.episode_rewards))):</span><br><span class="line">            dis_reward =  self.reward_decay * dis_reward +  self.episode_rewards[i]  # 前一步的reward等于后一步衰减reward加上即时奖励乘以衰减因子</span><br><span class="line">            out[i] = dis_reward</span><br><span class="line">        return  out/np.std(out - np.mean(out))</span><br><span class="line"></span><br><span class="line">    # 根据给定的状态选择对应的动作</span><br><span class="line">    def choose_action(self, state):</span><br><span class="line">        # 对状态的存储格式进行转换，便于神经网络的输入</span><br><span class="line">        state = state[ np.newaxis,:]</span><br><span class="line">        # 神经网络的前馈计算</span><br><span class="line">        # prob_actions = self.sess.run(self.outputs_softmax, feed_dict=&#123;self.X: state&#125;)</span><br><span class="line">        prob_actions = (self.model.predict(state))</span><br><span class="line">        </span><br><span class="line">        # print(prob_actions.ravel())</span><br><span class="line">        # 根据得到的动作概率随机选择一个作为需要执行的动作</span><br><span class="line">        action = np.random.choice(range(len(prob_actions.ravel())),p=prob_actions.ravel())</span><br><span class="line">        return action</span><br><span class="line">    </span><br><span class="line">    # 存储经验轨迹产生的数局作为后续神经网络的训练数据</span><br><span class="line">    def store_memory(self, state, action, reward):</span><br><span class="line"></span><br><span class="line">        # 记录状态数据</span><br><span class="line">        self.episode_states.append(state)</span><br><span class="line">        # 记录奖励数据</span><br><span class="line">        self.episode_rewards.append(reward)</span><br><span class="line">        # 创建动作空间</span><br><span class="line">        action__ = np.zeros(self.n_y)</span><br><span class="line">        # 当执行的动作设置为1其余为0</span><br><span class="line">        action__[action] = 1</span><br><span class="line">        # 创建动作空间</span><br><span class="line">        self.episode_actions.append(action__)</span><br><span class="line"></span><br><span class="line">    # 根据经验轨迹数据对神经网络进行训练</span><br><span class="line">    def learn(self):</span><br><span class="line">        # 奖励数据处理</span><br><span class="line">        self.disc_norm_ep_reward = self.__disc_and_norm_rewards()</span><br><span class="line">        # print(self.disc_norm_ep_reward)</span><br><span class="line">        # self.sess.run(self.trian_op, feed_dict=&#123;self.X: np.vstack(self.episode_states).T, self.Y: np.vstack(self.episode_actions).T, self.disc_norm_ep_reward: disc_norm_ep_reward,&#125;)</span><br><span class="line">        # print(&quot;X,&quot;,np.vstack(self.episode_states))</span><br><span class="line">        # print(&quot;Y,&quot;,np.vstack(self.episode_actions))</span><br><span class="line">        print(self.episode_states)</span><br><span class="line">        print(self.episode_actions.shape)</span><br><span class="line">        self.model.fit(np.vstack(self.episode_states),np.array(self.episode_actions),sample_weight=self.disc_norm_ep_reward)</span><br><span class="line">        # 重置经验轨迹数据用于记录下一条经验轨迹</span><br><span class="line">        self.episode_states, self.episode_actions, self.episode_rewards = [],[],[]</span><br></pre></td></tr></table></figure>

<p>总体而言，蒙特卡洛策略梯度法虽然数据无偏，但是存在着较大的噪音和误差，为能更加准确的估计动作值函数，我们引出了演员-评论家策略梯度算法。</p>
<h4 id="演员-评论家策略梯度算法"><a href="#演员-评论家策略梯度算法" class="headerlink" title="演员-评论家策略梯度算法"></a>演员-评论家策略梯度算法</h4><p>关于演员-评论家策略梯度算法，是结合了梯度策略法与值函数近似法为一体的策略梯度的算法，其中演员负责更新策略，评论家负责更新对应的动作值函数，详细代码如下：</p>
<p>关于演员的类：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"># 演员网络部分（Actor）</span><br><span class="line">class PolicyGradient():</span><br><span class="line">    # 策略梯度强化学习类，使用一个3层的神经网络作为策略网络</span><br><span class="line">    def __init__(self, n_x, n_y, learning_rate=0.01, reward_decay=0.95, load_path=None, save_path=None):</span><br><span class="line">        # 策略梯度类构造函数，初始化相关参数</span><br><span class="line"></span><br><span class="line">        # 初始化参数</span><br><span class="line">        # 策略网络输入</span><br><span class="line">        self.n_x = n_x</span><br><span class="line">        # 策略网络输出</span><br><span class="line">        self.n_y = n_y</span><br><span class="line">        # 策略网络学习率</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        # 策略网络奖励衰减率</span><br><span class="line">        self.reward_decay = reward_decay</span><br><span class="line">        # 经验轨迹采样数据（s，a，td_error）</span><br><span class="line">        self.episode_states, self.episode_actions, self.episode_tderrors = [],[],[]</span><br><span class="line">        # 建立策略网络</span><br><span class="line">        self.__build_network()</span><br><span class="line"></span><br><span class="line">    # 存储经验轨迹产生的数局作为后续神经网络的训练数据</span><br><span class="line">    def store_memory(self, state, action, td_error):</span><br><span class="line"></span><br><span class="line">        # 记录状态数据</span><br><span class="line">        self.episode_states.append(state)</span><br><span class="line">        # 记录奖励数据</span><br><span class="line">        self.episode_tderrors.append(td_error)</span><br><span class="line">        # 创建动作空间</span><br><span class="line">        action__ = np.zeros(self.n_y)</span><br><span class="line">        # 当执行的动作设置为1其余为0</span><br><span class="line">        action__[action] = 1</span><br><span class="line">        # 创建动作空间</span><br><span class="line">        self.episode_actions.append(action__)</span><br><span class="line"></span><br><span class="line">    # loss的定义   </span><br><span class="line">    def all_actf(self):</span><br><span class="line">        all_act = self.dense_out</span><br><span class="line">        # print(all_act)</span><br><span class="line">        return all_act</span><br><span class="line">    def reca_batch(self,a_batch):</span><br><span class="line">        a = a_batch</span><br><span class="line">        return a</span><br><span class="line">    def def_loss(self,label=reca_batch,logit=all_actf):  </span><br><span class="line">        neg_log_prob = tf.math.squared_difference(logit,label)</span><br><span class="line">        loss = tf.reduce_mean(neg_log_prob, self.episode_tderrors[0])</span><br><span class="line">        return neg_log_prob</span><br><span class="line"></span><br><span class="line">    def __build_network(self):</span><br><span class="line">        # 建立一个三层的神经网络</span><br><span class="line">        # 输入层</span><br><span class="line">        x = Input(shape= (self.n_x,))</span><br><span class="line">        # 定义层数的神经元</span><br><span class="line">        # 第一层的隐层神经元数</span><br><span class="line">        layer1_units = 10</span><br><span class="line">        # 第二层隐层神经元元数</span><br><span class="line">        layer2_unit = 10</span><br><span class="line">        # 输出层的神经元数</span><br><span class="line">        layer_output_units = self.n_y</span><br><span class="line">        # 定义第一层</span><br><span class="line">        dense1 = Dense(layer1_units, activation = &#x27;relu&#x27;, name=&#x27;layer1&#x27;)(x)</span><br><span class="line">        # 定义第二层</span><br><span class="line">        dense2 = Dense(layer2_unit, activation= &#x27;relu&#x27;, name=&#x27;layer2&#x27;)(dense1)</span><br><span class="line">        # 定义输出层</span><br><span class="line">        self.dense_out = Dense(layer_output_units,name=&#x27;out_layer&#x27;)(dense2)</span><br><span class="line">        # softmax 输出</span><br><span class="line">        self.outputs_softmax = Softmax()(self.dense_out)</span><br><span class="line"></span><br><span class="line">        self.model = Model(x,self.outputs_softmax)</span><br><span class="line"></span><br><span class="line">        # 定义神经网络的损失函数与训练方式</span><br><span class="line">        self.model.compile(optimizer=Adam(learning_rate=self.lr),loss=self.def_loss)</span><br><span class="line"></span><br><span class="line">        self.model.summary()</span><br><span class="line"></span><br><span class="line">    # 根据给定的状态选择对应的动作</span><br><span class="line">    def choose_action(self, state):</span><br><span class="line">        # 获得action的索引值</span><br><span class="line">        state = state[ np.newaxis,:]</span><br><span class="line">        # 神经网络的前馈计算</span><br><span class="line">        prob_actions = (self.model.predict(state))</span><br><span class="line">        </span><br><span class="line">        # 根据得到的动作概率随机选择一个作为需要执行的动作</span><br><span class="line">        action = np.random.choice(range(len(prob_actions.ravel())),p=prob_actions.ravel())</span><br><span class="line">        return action</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    # 根据经验轨迹数据对神经网络进行训练</span><br><span class="line">    def learn(self):</span><br><span class="line">        self.model.fit([np.vstack(self.episode_states)],[np.array(self.episode_actions)])</span><br><span class="line">        self.episode_states, self.episode_actions, self.episode_tderrors = [],[],[]</span><br></pre></td></tr></table></figure>

<p>关于评论家的类：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"># 评论家部分（Critic）</span><br><span class="line">class ValueEstimator():</span><br><span class="line">    def __init__(self, n_x ,n_y, learning_rate = 0.01, gama = 0.95 ,Load_path = None, Save_path=None):</span><br><span class="line">        </span><br><span class="line">        # 状态的尺寸</span><br><span class="line">        self.nS = n_x</span><br><span class="line">        # 动作值的尺寸</span><br><span class="line">        self.nR = n_y</span><br><span class="line">        # 学习率</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gama = 0.95</span><br><span class="line">        # 建立网络</span><br><span class="line">        self.critic_network()</span><br><span class="line">   </span><br><span class="line">    # loss的定义   </span><br><span class="line">    def all_actf(self):</span><br><span class="line">        all_act = self.dense_out</span><br><span class="line">        # print(all_act)</span><br><span class="line">        return all_act</span><br><span class="line">    def reca_batch(self,a_batch):</span><br><span class="line">        a = a_batch</span><br><span class="line">        return a</span><br><span class="line">    # label 为 真实 td_error         logit 为 self.denseout 的输出值</span><br><span class="line">    # r + gama * v_next - v</span><br><span class="line">    def def_loss(self,label=reca_batch,logit=all_actf):  </span><br><span class="line">        # loss = self.reward_now + self.gama * self.v_next - logit</span><br><span class="line">        # loss = tf.square(loss)</span><br><span class="line">        # return loss</span><br><span class="line">        neg_log_prob = tf.math.squared_difference(logit,label)</span><br><span class="line">        return neg_log_prob</span><br><span class="line"></span><br><span class="line">    # 建立神经网络</span><br><span class="line">    def critic_network(self):</span><br><span class="line">        x = Input(shape= (self.nS,))</span><br><span class="line">        # 定义层数的神经元</span><br><span class="line">        # 第一层的隐层神经元数</span><br><span class="line">        layer1_units = 20</span><br><span class="line">        # 输出层的神经元数</span><br><span class="line">        layer_output_units = self.nR</span><br><span class="line">        # 定义第一层</span><br><span class="line">        dense1 = Dense(layer1_units, activation = &#x27;relu&#x27;, name=&#x27;layer1&#x27;)(x)</span><br><span class="line">        # 定义输出层（输出的是td_error值）</span><br><span class="line">        self.dense_out = Dense(layer_output_units, name = &quot;output_layer&quot;)(dense1)</span><br><span class="line">        # 建立模型</span><br><span class="line">        self.model = Model(x,self.dense_out)</span><br><span class="line">        # 定义神经网络的损失函数与训练方式</span><br><span class="line">        self.model.compile(optimizer=Adam(learning_rate=self.lr),loss=self.def_loss)</span><br><span class="line"></span><br><span class="line">        self.model.summary()</span><br><span class="line">    </span><br><span class="line">    # 对状态值函数进行预测</span><br><span class="line">    def predict(self,s):</span><br><span class="line">        </span><br><span class="line">        s = s[np.newaxis,:]</span><br><span class="line">        # 预测该状态函数</span><br><span class="line">        prob_weights = self.model.predict(s)</span><br><span class="line">        # 返回状态函数的值</span><br><span class="line">        return prob_weights[0]</span><br><span class="line"></span><br><span class="line">    # 对神经网络进行训练</span><br><span class="line">    def learn(self, s, td_target ):</span><br><span class="line">        # 创建动作空间</span><br><span class="line">        s = s[np.newaxis, :]</span><br><span class="line">        self.model.fit(np.vstack(s),np.array(td_target))</span><br></pre></td></tr></table></figure>

<p>对于算法流程的整体同样也可以分为6个步骤，详细如下：</p>
<ol>
<li>根据演员网络选择动作</li>
<li>环境执行演员网络选择的动作并得到新的状态和奖励信号</li>
<li>利用新的状态信号作为评论家的输入，计算时间差分误差和时间差分目标</li>
<li>利用时间差分目标更新评论家网络中的参数</li>
<li>利用时间差分误差更新演员网络中的参数</li>
<li>对下一时间步的状态进行赋值</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"># 演员评论家策略梯度类</span><br><span class="line">class Actor_Critic():</span><br><span class="line">    # 演员-评论家策略梯度方法类</span><br><span class="line">    def __init__(self, env, num_episodes=200, learning_rate = 0.01, reward_decay = 0.95):</span><br><span class="line">        # 初始化参数</span><br><span class="line">        # 动作空间</span><br><span class="line">        self.nA = env.action_space.n</span><br><span class="line">        # 状态空间</span><br><span class="line">        self.nS = env.observation_space.shape[0]</span><br><span class="line">        # </span><br><span class="line">        self.nR = 1</span><br><span class="line">        # 声明环境</span><br><span class="line">        self.env = env</span><br><span class="line">        # 迭代次数</span><br><span class="line">        self.num_episodes = num_episodes</span><br><span class="line">        # 奖励衰减系数</span><br><span class="line">        self.reward_decay = reward_decay</span><br><span class="line">        # 网络学习率</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        # 记录所有的奖励</span><br><span class="line">        self.rewards = []</span><br><span class="line">        # 最小奖励阈值</span><br><span class="line">        self.RENDER_REWARD_MIN = 30</span><br><span class="line">        # 是否重新分配环境标志位</span><br><span class="line">        self.RENDER_ENV = False</span><br><span class="line"></span><br><span class="line">        # 初始化策略网络类</span><br><span class="line">        self.Actor = PolicyGradient(n_x=self.nS, n_y = self.nA, learning_rate = self.learning_rate, reward_decay = self.reward_decay)</span><br><span class="line">        self.Critic = ValueEstimator(n_x=self.nS, n_y = self.nR, learning_rate = self.learning_rate)</span><br><span class="line">        # 记录经验轨迹的长度和奖励</span><br><span class="line">        record_head = namedtuple(&quot;Stats&quot;, [&quot;episode_lengths&quot;,&quot;episode_rewards&quot;])</span><br><span class="line">        self.record = record_head(episode_lengths = np.zeros(num_episodes), episode_rewards = np.zeros(num_episodes))</span><br><span class="line">    </span><br><span class="line">    # 演员-评论家策略梯度算法核心代码</span><br><span class="line">    def ac_learn(self):</span><br><span class="line">        # 迭代经验轨迹的次数</span><br><span class="line">        for i_episode in range(self.num_episodes):</span><br><span class="line">            # 输出经验轨迹迭代信息</span><br><span class="line">            num_present = (i_episode+1) / self.num_episodes</span><br><span class="line">            print(&quot;Episode &#123;&#125; / &#123;&#125;&quot;.format(i_episode+1,self.num_episodes), end=&quot;&quot;)</span><br><span class="line">            # 信息输出</span><br><span class="line">            print(&quot;=&quot;*round(num_present*60))</span><br><span class="line"></span><br><span class="line">            # 初始化环境</span><br><span class="line">            # 环境reset</span><br><span class="line">            state = env.reset()</span><br><span class="line">            </span><br><span class="line">            # 初始化奖励为0</span><br><span class="line">            reward_ = 0</span><br><span class="line"></span><br><span class="line">            # 遍历经验轨迹</span><br><span class="line">            for t in itertools.count():</span><br><span class="line">                </span><br><span class="line">                # 如果环境重置标志位为True，则对环境重置</span><br><span class="line">                if self.RENDER_ENV: env.render()</span><br><span class="line"></span><br><span class="line">                # 步骤1：根据策略网络（Actor网络）选择出相应的动作</span><br><span class="line">                action = self.Actor.choose_action(state)</span><br><span class="line">                # 步骤2：环境执行动作给出反馈信号</span><br><span class="line">                next_state, reward, done, _ = env.step(action)</span><br><span class="line">                print(&quot;state:&quot;, next_state,&quot;reward:&quot;,reward, &quot;action:&quot;,action)</span><br><span class="line">                # 更新奖励</span><br><span class="line">                reward_ +=reward </span><br><span class="line"></span><br><span class="line">                # 步骤3：记录时间差分误差</span><br><span class="line">                # 评论家预测的下一个状态价值</span><br><span class="line">                value_next = self.Critic.predict(next_state)</span><br><span class="line">                # 计算时间差分目标</span><br><span class="line">                td_target = reward + self.reward_decay * value_next</span><br><span class="line">                # 计算时间差分误差</span><br><span class="line">                td_error = td_target - self.Critic.predict(state)</span><br><span class="line">                # self.Actor.store_transition(td_error)</span><br><span class="line">                print(&quot;value_next:&quot;,value_next,&quot;TD_targrt:&quot;,td_target)</span><br><span class="line">                # 步骤4：更新价值网络（评议家网络）</span><br><span class="line">                self.Critic.learn(state, td_target)</span><br><span class="line">                # 步骤5：更新策略网络（演员网络）</span><br><span class="line">                self.Actor.store_memory(state, action, td_error)</span><br><span class="line">                self.Actor.learn()</span><br><span class="line"></span><br><span class="line">                # 更新记录的信息</span><br><span class="line">                self.record.episode_rewards[i_episode] += reward</span><br><span class="line">                self.record.episode_lengths[i_episode] = t</span><br><span class="line"></span><br><span class="line">                # 游戏结束</span><br><span class="line">                if done:</span><br><span class="line">                    # 计算本次经验轨迹所获得的累计奖励</span><br><span class="line">                    self.rewards.append(reward_)</span><br><span class="line">                    # </span><br><span class="line">                    max_reward = np.max(self.rewards)</span><br><span class="line"></span><br><span class="line">                    # 历史最大奖励信息，标准化输出信息</span><br><span class="line">                    print(&quot;reward:&#123;&#125;,max reward:&#123;&#125;, episode:&#123;&#125;\n&quot;.format(reward_, max_reward, t))</span><br><span class="line"></span><br><span class="line">                    # 如果历史最大奖励大于奖励的最小阈值，则重置环境标志位为True</span><br><span class="line">                    if max_reward &gt; self.RENDER_REWARD_MIN : self.RENDER_ENV = True</span><br><span class="line"></span><br><span class="line">                    # 退出本次经验轨迹</span><br><span class="line">                    break</span><br><span class="line">                # 步骤5：存储下一个状态作为新的状态记录</span><br><span class="line">                state = next_state</span><br><span class="line">        # 返回记录数据</span><br><span class="line">        return self.record            </span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/08/policyGradient/" data-id="ckrc6fps60002b8uogcjoc9g0" data-title="强化学习求解（五） ———— 策略梯度法" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/07/20/hello-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
  
    <a href="/2021/02/23/value_function_approximate/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">强化学习求解（四） ———— 值函数近似法</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/07/20/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2021/03/08/policyGradient/">强化学习求解（五） ———— 策略梯度法</a>
          </li>
        
          <li>
            <a href="/2021/02/23/value_function_approximate/">强化学习求解（四） ———— 值函数近似法</a>
          </li>
        
          <li>
            <a href="/2021/02/17/temporal_difference/">强化学习求解（三） ———— 时间差分法</a>
          </li>
        
          <li>
            <a href="/2020/12/08/montocarlo/">强化学习求解（二） ———— 蒙特卡洛法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>